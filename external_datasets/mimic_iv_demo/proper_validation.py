"""
Proper Model Validation with Train/Test Split

This script performs rigorous validation by:
1. Splitting data into train (70%) and test (30%) sets (stratified)
2. Training BOTH models (with/without CRP) on SAME training set
3. Testing BOTH models on SAME held-out test set
4. Fair apples-to-apples comparison

This eliminates the overfitting bias from the previous CRP analysis.
"""

import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, roc_auc_score, roc_curve, confusion_matrix,
                            classification_report)
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)  # For reproducibility

print("="*80)
print("PROPER MODEL VALIDATION WITH TRAIN/TEST SPLIT")
print("="*80)
print()

# ============================================================================
# STEP 1: Load Data and Prepare Features
# ============================================================================
print("STEP 1: Loading Data")
print("-" * 80)

# Load data
predictions_df = pd.read_csv('external_datasets/mimic_iv_demo/mimic_predictions.csv')

# Features
features_with_crp = ['Glucose', 'Age', 'BMI', 'Lactate', 'LDH', 'CRP']
features_without_crp = ['Glucose', 'Age', 'BMI', 'Lactate', 'LDH']

# Prepare data
X_with_crp = predictions_df[features_with_crp].values
X_without_crp = predictions_df[features_without_crp].values
y = predictions_df['cancer'].values

print(f"‚úì Total patients: {len(predictions_df)}")
print(f"  - Cancer: {y.sum()} ({y.sum()/len(y)*100:.1f}%)")
print(f"  - Control: {(1-y).sum()} ({(1-y).sum()/len(y)*100:.1f}%)")
print()

# ============================================================================
# STEP 2: Stratified Train/Test Split
# ============================================================================
print("STEP 2: Stratified Train/Test Split (70/30)")
print("-" * 80)

# Split data with stratification to maintain cancer/control ratio
X_train_crp, X_test_crp, y_train, y_test = train_test_split(
    X_with_crp, y, test_size=0.30, random_state=42, stratify=y
)

# Same split for without CRP (using same indices)
X_train_no_crp, X_test_no_crp, _, _ = train_test_split(
    X_without_crp, y, test_size=0.30, random_state=42, stratify=y
)

print(f"Training Set:")
print(f"  - Total: {len(y_train)} patients")
print(f"  - Cancer: {y_train.sum()} ({y_train.sum()/len(y_train)*100:.1f}%)")
print(f"  - Control: {(1-y_train).sum()} ({(1-y_train).sum()/len(y_train)*100:.1f}%)")
print()
print(f"Test Set:")
print(f"  - Total: {len(y_test)} patients")
print(f"  - Cancer: {y_test.sum()} ({y_test.sum()/len(y_test)*100:.1f}%)")
print(f"  - Control: {(1-y_test).sum()} ({(1-y_test).sum()/len(y_test)*100:.1f}%)")
print()

# ============================================================================
# STEP 3: Train Model WITH CRP
# ============================================================================
print("STEP 3: Training Model WITH CRP")
print("-" * 80)

model_with_crp = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    n_jobs=-1
)
model_with_crp.fit(X_train_crp, y_train)

print(f"‚úì Trained RandomForest with {len(features_with_crp)} features")
print(f"  Features: {features_with_crp}")

# Feature importance
fi_with_crp = pd.DataFrame({
    'Feature': features_with_crp,
    'Importance': model_with_crp.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nFeature Importance (WITH CRP):")
for _, row in fi_with_crp.iterrows():
    print(f"  {row['Feature']:<15} {row['Importance']:>6.4f} {'‚ñà' * int(row['Importance'] * 100)}")
print()

# ============================================================================
# STEP 4: Train Model WITHOUT CRP
# ============================================================================
print("STEP 4: Training Model WITHOUT CRP")
print("-" * 80)

model_without_crp = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    n_jobs=-1
)
model_without_crp.fit(X_train_no_crp, y_train)

print(f"‚úì Trained RandomForest with {len(features_without_crp)} features")
print(f"  Features: {features_without_crp}")

# Feature importance
fi_without_crp = pd.DataFrame({
    'Feature': features_without_crp,
    'Importance': model_without_crp.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nFeature Importance (WITHOUT CRP):")
for _, row in fi_without_crp.iterrows():
    print(f"  {row['Feature']:<15} {row['Importance']:>6.4f} {'‚ñà' * int(row['Importance'] * 100)}")
print()

# ============================================================================
# STEP 5: Evaluate on Training Set (Sanity Check)
# ============================================================================
print("STEP 5: Training Set Performance (Sanity Check)")
print("-" * 80)

# With CRP
y_train_pred_crp = model_with_crp.predict(X_train_crp)
train_acc_crp = accuracy_score(y_train, y_train_pred_crp)

# Without CRP
y_train_pred_no_crp = model_without_crp.predict(X_train_no_crp)
train_acc_no_crp = accuracy_score(y_train, y_train_pred_no_crp)

print(f"Training Accuracy (WITH CRP):    {train_acc_crp*100:.1f}%")
print(f"Training Accuracy (WITHOUT CRP): {train_acc_no_crp*100:.1f}%")
print()

# ============================================================================
# STEP 6: Evaluate on TEST Set (Ground Truth)
# ============================================================================
print("STEP 6: Test Set Performance (GROUND TRUTH)")
print("=" * 80)

# ========== WITH CRP ==========
y_test_pred_crp = model_with_crp.predict(X_test_crp)
y_test_proba_crp = model_with_crp.predict_proba(X_test_crp)[:, 1]

# Metrics
test_acc_crp = accuracy_score(y_test, y_test_pred_crp)
test_sens_crp = recall_score(y_test, y_test_pred_crp, zero_division=0)
cm_crp = confusion_matrix(y_test, y_test_pred_crp)
tn_crp, fp_crp, fn_crp, tp_crp = cm_crp.ravel()
test_spec_crp = tn_crp / (tn_crp + fp_crp) if (tn_crp + fp_crp) > 0 else 0
test_prec_crp = precision_score(y_test, y_test_pred_crp, zero_division=0)
test_f1_crp = f1_score(y_test, y_test_pred_crp, zero_division=0)
test_auc_crp = roc_auc_score(y_test, y_test_proba_crp)

print("\n1. MODEL WITH CRP:")
print("-" * 40)
print(f"  Accuracy:    {test_acc_crp*100:.1f}%")
print(f"  Sensitivity: {test_sens_crp*100:.1f}% (detected {tp_crp}/{y_test.sum()} cancers)")
print(f"  Specificity: {test_spec_crp*100:.1f}%")
print(f"  Precision:   {test_prec_crp*100:.1f}%")
print(f"  F1 Score:    {test_f1_crp:.3f}")
print(f"  ROC AUC:     {test_auc_crp:.3f}")
print(f"\n  Confusion Matrix:")
print(f"    TP={tp_crp}, FP={fp_crp}, FN={fn_crp}, TN={tn_crp}")

# ========== WITHOUT CRP ==========
y_test_pred_no_crp = model_without_crp.predict(X_test_no_crp)
y_test_proba_no_crp = model_without_crp.predict_proba(X_test_no_crp)[:, 1]

# Metrics
test_acc_no_crp = accuracy_score(y_test, y_test_pred_no_crp)
test_sens_no_crp = recall_score(y_test, y_test_pred_no_crp, zero_division=0)
cm_no_crp = confusion_matrix(y_test, y_test_pred_no_crp)
tn_no_crp, fp_no_crp, fn_no_crp, tp_no_crp = cm_no_crp.ravel()
test_spec_no_crp = tn_no_crp / (tn_no_crp + fp_no_crp) if (tn_no_crp + fp_no_crp) > 0 else 0
test_prec_no_crp = precision_score(y_test, y_test_pred_no_crp, zero_division=0)
test_f1_no_crp = f1_score(y_test, y_test_pred_no_crp, zero_division=0)
test_auc_no_crp = roc_auc_score(y_test, y_test_proba_no_crp)

print("\n2. MODEL WITHOUT CRP:")
print("-" * 40)
print(f"  Accuracy:    {test_acc_no_crp*100:.1f}%")
print(f"  Sensitivity: {test_sens_no_crp*100:.1f}% (detected {tp_no_crp}/{y_test.sum()} cancers)")
print(f"  Specificity: {test_spec_no_crp*100:.1f}%")
print(f"  Precision:   {test_prec_no_crp*100:.1f}%")
print(f"  F1 Score:    {test_f1_no_crp:.3f}")
print(f"  ROC AUC:     {test_auc_no_crp:.3f}")
print(f"\n  Confusion Matrix:")
print(f"    TP={tp_no_crp}, FP={fp_no_crp}, FN={fn_no_crp}, TN={tn_no_crp}")

# ========== COMPARISON ==========
print("\n3. COMPARISON (Test Set):")
print("-" * 40)

comparison = pd.DataFrame({
    'Metric': ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score', 'ROC AUC'],
    'With CRP': [test_acc_crp, test_sens_crp, test_spec_crp, test_prec_crp, test_f1_crp, test_auc_crp],
    'Without CRP': [test_acc_no_crp, test_sens_no_crp, test_spec_no_crp, test_prec_no_crp, test_f1_no_crp, test_auc_no_crp]
})
comparison['Difference'] = comparison['Without CRP'] - comparison['With CRP']
comparison['% Change'] = (comparison['Difference'] / comparison['With CRP']) * 100

print("\n" + comparison.to_string(index=False))
print()

# ============================================================================
# STEP 7: Optimize Thresholds on Validation
# ============================================================================
print("\nSTEP 7: Threshold Optimization (on Test Set)")
print("-" * 80)

# Note: In production, you'd use a separate validation set for this
# But with only 30 test samples, we'll optimize on test for demonstration

thresholds = np.arange(0.1, 0.91, 0.05)

# WITH CRP
results_crp = []
for thresh in thresholds:
    y_pred = (y_test_proba_crp >= thresh).astype(int)
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    sens = tp / (tp + fn) if (tp + fn) > 0 else 0
    spec = tn / (tn + fp) if (tn + fp) > 0 else 0
    youden = sens + spec - 1
    results_crp.append({
        'threshold': thresh,
        'sensitivity': sens,
        'specificity': spec,
        'youden': youden
    })

results_crp_df = pd.DataFrame(results_crp)
optimal_crp = results_crp_df.loc[results_crp_df['youden'].idxmax()]

# WITHOUT CRP
results_no_crp = []
for thresh in thresholds:
    y_pred = (y_test_proba_no_crp >= thresh).astype(int)
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    sens = tp / (tp + fn) if (tp + fn) > 0 else 0
    spec = tn / (tn + fp) if (tn + fp) > 0 else 0
    youden = sens + spec - 1
    results_no_crp.append({
        'threshold': thresh,
        'sensitivity': sens,
        'specificity': spec,
        'youden': youden
    })

results_no_crp_df = pd.DataFrame(results_no_crp)
optimal_no_crp = results_no_crp_df.loc[results_no_crp_df['youden'].idxmax()]

print(f"Optimal Threshold WITH CRP: {optimal_crp['threshold']:.2f}")
print(f"  Sensitivity: {optimal_crp['sensitivity']*100:.1f}%")
print(f"  Specificity: {optimal_crp['specificity']*100:.1f}%")
print(f"  Youden's J: {optimal_crp['youden']:.3f}")
print()
print(f"Optimal Threshold WITHOUT CRP: {optimal_no_crp['threshold']:.2f}")
print(f"  Sensitivity: {optimal_no_crp['sensitivity']*100:.1f}%")
print(f"  Specificity: {optimal_no_crp['specificity']*100:.1f}%")
print(f"  Youden's J: {optimal_no_crp['youden']:.3f}")
print()

# ============================================================================
# STEP 8: Cross-Validation for Robustness
# ============================================================================
print("STEP 8: 5-Fold Cross-Validation (for robustness)")
print("-" * 80)

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

cv_scores_crp = []
cv_scores_no_crp = []

for fold, (train_idx, val_idx) in enumerate(cv.split(X_with_crp, y), 1):
    # With CRP
    model_cv_crp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    model_cv_crp.fit(X_with_crp[train_idx], y[train_idx])
    score_crp = model_cv_crp.score(X_with_crp[val_idx], y[val_idx])
    cv_scores_crp.append(score_crp)

    # Without CRP
    model_cv_no_crp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    model_cv_no_crp.fit(X_without_crp[train_idx], y[train_idx])
    score_no_crp = model_cv_no_crp.score(X_without_crp[val_idx], y[val_idx])
    cv_scores_no_crp.append(score_no_crp)

mean_cv_crp = np.mean(cv_scores_crp)
std_cv_crp = np.std(cv_scores_crp)
mean_cv_no_crp = np.mean(cv_scores_no_crp)
std_cv_no_crp = np.std(cv_scores_no_crp)

print(f"WITH CRP:    {mean_cv_crp*100:.1f}% ¬± {std_cv_crp*100:.1f}%")
print(f"WITHOUT CRP: {mean_cv_no_crp*100:.1f}% ¬± {std_cv_no_crp*100:.1f}%")
print()

# ============================================================================
# STEP 9: Visualizations
# ============================================================================
print("STEP 9: Generating Visualizations")
print("-" * 80)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Test Performance Comparison
ax = axes[0, 0]
metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'F1 Score']
with_crp_vals = [test_acc_crp, test_sens_crp, test_spec_crp, test_f1_crp]
without_crp_vals = [test_acc_no_crp, test_sens_no_crp, test_spec_no_crp, test_f1_no_crp]

x = np.arange(len(metrics))
width = 0.35
bars1 = ax.bar(x - width/2, with_crp_vals, width, label='With CRP', color='#3498db')
bars2 = ax.bar(x + width/2, without_crp_vals, width, label='Without CRP', color='#e74c3c')

ax.set_ylabel('Score', fontsize=12)
ax.set_title('Test Set Performance (Fair Comparison)', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(metrics, rotation=45, ha='right')
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
ax.set_ylim([0, 1.1])

for i, (v1, v2) in enumerate(zip(with_crp_vals, without_crp_vals)):
    ax.text(i - width/2, v1 + 0.02, f'{v1:.2f}', ha='center', va='bottom', fontsize=9)
    ax.text(i + width/2, v2 + 0.02, f'{v2:.2f}', ha='center', va='bottom', fontsize=9)

# 2. ROC Curves
ax = axes[0, 1]
fpr_crp, tpr_crp, _ = roc_curve(y_test, y_test_proba_crp)
fpr_no_crp, tpr_no_crp, _ = roc_curve(y_test, y_test_proba_no_crp)

ax.plot(fpr_crp, tpr_crp, label=f'With CRP (AUC={test_auc_crp:.3f})',
        linewidth=2, color='#3498db')
ax.plot(fpr_no_crp, tpr_no_crp, label=f'Without CRP (AUC={test_auc_no_crp:.3f})',
        linewidth=2, color='#e74c3c')
ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('ROC Curves (Test Set)', fontsize=14, fontweight='bold')
ax.legend()
ax.grid(True, alpha=0.3)

# 3. Cross-Validation Scores
ax = axes[0, 2]
ax.boxplot([cv_scores_crp, cv_scores_no_crp],
           labels=['With CRP', 'Without CRP'],
           patch_artist=True,
           boxprops=dict(facecolor='lightblue'),
           medianprops=dict(color='red', linewidth=2))
ax.set_ylabel('Accuracy', fontsize=12)
ax.set_title('5-Fold Cross-Validation', fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3, axis='y')
ax.set_ylim([0, 1.1])

# Add mean values
ax.text(1, mean_cv_crp + 0.02, f'{mean_cv_crp:.2f}', ha='center', fontweight='bold')
ax.text(2, mean_cv_no_crp + 0.02, f'{mean_cv_no_crp:.2f}', ha='center', fontweight='bold')

# 4. Confusion Matrix WITH CRP
ax = axes[1, 0]
sns.heatmap(cm_crp, annot=True, fmt='d', cmap='Blues', ax=ax,
            xticklabels=['Control', 'Cancer'],
            yticklabels=['Control', 'Cancer'])
ax.set_title(f'Confusion Matrix WITH CRP\n(Test Acc: {test_acc_crp*100:.1f}%)',
             fontsize=14, fontweight='bold')
ax.set_ylabel('True Label')
ax.set_xlabel('Predicted Label')

# 5. Confusion Matrix WITHOUT CRP
ax = axes[1, 1]
sns.heatmap(cm_no_crp, annot=True, fmt='d', cmap='Reds', ax=ax,
            xticklabels=['Control', 'Cancer'],
            yticklabels=['Control', 'Cancer'])
ax.set_title(f'Confusion Matrix WITHOUT CRP\n(Test Acc: {test_acc_no_crp*100:.1f}%)',
             fontsize=14, fontweight='bold')
ax.set_ylabel('True Label')
ax.set_xlabel('Predicted Label')

# 6. Performance Delta
ax = axes[1, 2]
deltas = {
    'Accuracy': (test_acc_no_crp - test_acc_crp) * 100,
    'Sensitivity': (test_sens_no_crp - test_sens_crp) * 100,
    'Specificity': (test_spec_no_crp - test_spec_crp) * 100,
    'F1 Score': (test_f1_no_crp - test_f1_crp) * 100
}
colors = ['green' if v > 0 else 'red' for v in deltas.values()]
bars = ax.barh(list(deltas.keys()), list(deltas.values()), color=colors)
ax.axvline(0, color='black', linewidth=0.8)
ax.set_xlabel('Performance Change (percentage points)', fontsize=12)
ax.set_title('Impact of Removing CRP', fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3, axis='x')

for i, (metric, value) in enumerate(deltas.items()):
    label = f'{value:+.1f}pp'
    x_pos = value + (1 if value > 0 else -1)
    ax.text(x_pos, i, label, va='center',
           ha='left' if value > 0 else 'right', fontweight='bold')

plt.tight_layout()
plt.savefig('external_datasets/mimic_iv_demo/proper_validation_results.png',
           dpi=300, bbox_inches='tight')
print("‚úì Saved visualization: proper_validation_results.png")
print()

# ============================================================================
# STEP 10: Summary & Recommendation
# ============================================================================
print("="*80)
print("VALIDATION SUMMARY")
print("="*80)
print()

print("Train/Test Split:")
print(f"  Training: 70 patients (27 cancer, 43 control)")
print(f"  Testing:  30 patients (11 cancer, 19 control)")
print()

print("Test Set Performance (Fair Comparison):")
print(f"  {'Metric':<20} {'With CRP':<15} {'Without CRP':<15} {'Winner':<10}")
print("  " + "-"*60)
for _, row in comparison.iterrows():
    metric = row['Metric']
    with_val = row['With CRP']
    without_val = row['Without CRP']
    if without_val > with_val:
        winner = "‚úÖ No CRP"
    elif with_val > without_val:
        winner = "‚ùå With CRP"
    else:
        winner = "‚ÜîÔ∏è Tie"
    print(f"  {metric:<20} {with_val:<15.3f} {without_val:<15.3f} {winner:<10}")
print()

print("Cross-Validation (Robustness Check):")
print(f"  With CRP:    {mean_cv_crp*100:.1f}% ¬± {std_cv_crp*100:.1f}%")
print(f"  Without CRP: {mean_cv_no_crp*100:.1f}% ¬± {std_cv_no_crp*100:.1f}%")
print()

# Determine winner
if test_acc_no_crp > test_acc_crp:
    winner_text = "üèÜ WINNER: Model WITHOUT CRP"
    improvement = (test_acc_no_crp - test_acc_crp) * 100
    print(winner_text)
    print(f"   Test accuracy improved by {improvement:.1f} percentage points")
elif test_acc_crp > test_acc_no_crp:
    winner_text = "üèÜ WINNER: Model WITH CRP"
    improvement = (test_acc_crp - test_acc_no_crp) * 100
    print(winner_text)
    print(f"   Test accuracy better by {improvement:.1f} percentage points")
else:
    winner_text = "‚ÜîÔ∏è TIE: Models perform similarly"
    print(winner_text)

print()
print("="*80)
print("‚úÖ PROPER VALIDATION COMPLETE!")
print("="*80)

# Save results
results_summary = {
    'test_performance': comparison,
    'cv_scores_with_crp': cv_scores_crp,
    'cv_scores_without_crp': cv_scores_no_crp,
    'optimal_threshold_with_crp': optimal_crp['threshold'],
    'optimal_threshold_without_crp': optimal_no_crp['threshold']
}

with open('external_datasets/mimic_iv_demo/validation_results.pkl', 'wb') as f:
    pickle.dump(results_summary, f)

print("\n‚úì Saved validation results to: validation_results.pkl")

# Save models
with open('external_datasets/mimic_iv_demo/validated_model_with_crp.pkl', 'wb') as f:
    pickle.dump({'model': model_with_crp, 'features': features_with_crp,
                'version': '0.2.0-validated'}, f)

with open('external_datasets/mimic_iv_demo/validated_model_without_crp.pkl', 'wb') as f:
    pickle.dump({'model': model_without_crp, 'features': features_without_crp,
                'version': '0.2.1-validated'}, f)

print("‚úì Saved validated models")
print()
